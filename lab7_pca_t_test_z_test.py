# -*- coding: utf-8 -*-
"""lab7 - PCA t-test z-test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1heaszspGLU_DYaeilfqowKQR7wunvTsB
"""

from sklearn.datasets import load_digits
import pandas as pd

dataset = load_digits()
# dataset.keys()
print(dataset.keys())
dataset.data.shape

# Commented out IPython magic to ensure Python compatibility.
from matplotlib import pyplot as plt
# %matplotlib inline
plt.gray()
plt.matshow(dataset.data[0].reshape(8,8))

plt.matshow(dataset.data[9].reshape(8,8))

dataset.target[:5]

df = pd.DataFrame(dataset.data, columns=dataset.feature_names)
df.head()

df.describe()

"""### **prediction without PCA**"""

X = df
y = dataset.target

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()  #  creating the model
model.fit(X_train, y_train)
model.score(X_test, y_test)

"""### **Prediction using PCA**"""

X

from sklearn.decomposition import PCA

1pca = PCA(0.97)                   # 95 % variance retains
X_pca = pca.fit_transform(X)
X_pca.shape

pca.explained_variance_ratio_

pca.n_components_

X_pca

X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=1000)
model.fit(X_train_pca, y_train)
model.score(X_test_pca, y_test)

"""### **with different number of components**"""

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
X_pca.shape

X_pca

pca.explained_variance_ratio_

X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)

model = LogisticRegression(max_iter=1000)
model.fit(X_train_pca, y_train)
model.score(X_test_pca, y_test)

"""### **T-test**"""

import numpy as np
from scipy import stats

# Define the dataset
scores = [78, 85, 72, 90, 83, 80, 77, 75, 70, 88]

# Define the null hypothesis (H0): mean score = 75
mu = 75

# Define the alternative hypothesis (HA): mean score != 75
alpha = 0.05
ha = 'two-sided'

# Calculate the sample mean and standard deviation
x_bar = np.mean(scores)
s = np.std(scores, ddof=1) # use ddof=1 for unbiased estimator

# Calculate the test statistic (t-score)
t = (x_bar - mu) / (s / np.sqrt(len(scores)))

# Calculate the degrees of freedom
df = len(scores) - 1

# Calculate the p-value
p_value = stats.t.sf(abs(t), df) * 2 # two-sided test

# Compare the p-value with the significance level
if p_value < alpha:
    print("Reject the null hypothesis")
else:
    print("Fail to reject the null hypothesis")

"""### **z-test**"""

import numpy as np
from scipy import stats

# Define the dataset
scores = [12, 65, 88, 90, 45, 64, 75, 35, 95, 100]

# Define the null hypothesis (H0): mean score = 75
mu = 0.05

# Define the alternative hypothesis (HA): mean score != 75
alpha = 50
ha = 'two-sided'

# Calculate the sample mean and standard deviation
x_bar = np.mean(scores)
s = np.std(scores, ddof=1) # use ddof=1 for unbiased estimator

# Calculate the test statistic (z-score)
z = (x_bar - mu) / (s / np.sqrt(len(scores)))

# Calculate the p-value
p_value = stats.norm.sf(abs(z)) * 2 # two-sided test

# Compare the p-value with the significance level
if p_value < alpha:
    print("Reject the null hypothesis")
else:
    print("Fail to reject the null hypothesis")